"""
Critique and Refine with Examples Module

This standalone module implements the "Feedback-Driven Refinement" feature from PromptWizard.
It includes the core logic for generating critiques, refining prompts based on feedback,
and working with examples to improve prompt performance.
"""

import os
import re
import random
import json
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod
import openai
from tenacity import retry, stop_after_attempt, wait_fixed

# Import shared utilities
from promptwizard.prompt_opt_utils import (
    LLMManager, DataProcessor, BasePromptGenerator,
    extract_between, TEXT_DELIMITER_PATTERN, ANSWER_START, ANSWER_END,
    ANSWER_DELIMITER_PATTERN, QUESTION_LITERAL, ANSWER_WITH_REASON_LITERAL,
    FINAL_ANSWER_LITERAL, QUESTION_KEY_IN_PROMPT, ANSWER_KEY_IN_PROMPT
)

# ===============================================================================
# Base Classes
# ===============================================================================

class DatasetSpecificProcessing(ABC):
    """
    Base class for dataset-specific processing requirements.
    This class should be inherited and its methods defined based on the specific dataset and use case.
    """
    QUESTION_LITERAL = QUESTION_LITERAL
    ANSWER_WITH_REASON_LITERAL = ANSWER_WITH_REASON_LITERAL
    FINAL_ANSWER_LITERAL = FINAL_ANSWER_LITERAL
    QUESTION_KEY_IN_PROMPT = QUESTION_KEY_IN_PROMPT
    ANSWER_KEY_IN_PROMPT = ANSWER_KEY_IN_PROMPT
    # Regular expression pattern to match text between <START> and <END> tags
    TEXT_DELIMITER_PATTERN = TEXT_DELIMITER_PATTERN
    TEXT_DELIMITER_PATTERN_MUTATION = TEXT_DELIMITER_PATTERN
    ANSWER_START = ANSWER_START
    ANSWER_END = ANSWER_END
    ANSWER_DELIMITER_PATTERN = ANSWER_DELIMITER_PATTERN
    INVALID_ANS = "[invalid]"
    
    def normalize_prediction(self, prediction, lowercase=True):
        """Normalize a prediction string for comparison."""
        import re
        import string
        prediction = prediction.replace(' and ', ' ')
        prediction = prediction.replace('Sentence 1:', ' ')
        prediction = prediction.replace('Sentence 2:', ' ')
        prediction = prediction.strip()
        prediction = prediction.split("\n")[0]
        prediction = prediction.split(".")[0]

        if lowercase:
            prediction = prediction.lower()

        # remove punctuation
        prediction = prediction.replace('-', ' ')
        prediction = prediction.translate(
            str.maketrans('', '', string.punctuation))

        return prediction
    
    def access_answer(self, llm_output: str, gt_answer: str) -> Tuple[bool, Any]:
        """
        Compare answer generated by model with the answer in ground truth.
        Return True if they are equal. Definition of 'equal' depends on problem at hand.

        :param llm_output: Output of LLM i.e. the predicted answer
        :param gt_answer: The expected ground truth answer
        """
        predicted_answer = self.extract_final_answer(llm_output)
        is_correct = False
        if predicted_answer and (predicted_answer.lower() == gt_answer.lower()):
            is_correct = True

        return is_correct, predicted_answer

    def collate_to_str(self, examples: List, example_template: str) -> str:
        """
        Take as input a list of examples. Populate common template with values in these examples.
        Concatenate all of them to a single string, which can then be passed to LLM as prompt.

        :param examples: List of examples
        :param example_template: A template of giving examples to LLM as part of few shot learning
        :return: Concatenated string of all examples over the template.
        """
        example_string = ""
        for example in examples:
            answer = example[self.FINAL_ANSWER_LITERAL]
            if self.ANSWER_WITH_REASON_LITERAL in example:
                answer = example[self.ANSWER_WITH_REASON_LITERAL]

            example_string += example_template.format(question=example[self.QUESTION_LITERAL],
                                                     answer=answer)
        return example_string

    def extract_final_answer(self, answer: str) -> str:
        """
        Parse the output of LLM and extract the answer that you need from it.
        This method should be overridden & custom defined based on end use-case.

        :param answer: Output of LLM i.e. the response the to the question asked.
        :return: Final answer extracted from `answer` text, that we are looking for.
        """
        return answer

@dataclass
class PromptOptimizationParams:
    """Parameters for prompt optimization."""
    # Number of candidate prompts to generate in given iteration
    style_variation: int
    # Number of questions to be asked to LLM in a single go
    questions_batch_size: int
    # Number of batches of questions to correctly answered, for a prompt to be considered as performing good
    min_correct_count: int
    # Max number of mini-batches on which we should evaluate our prompt
    max_eval_batches: int
    # Number of top best performing prompts to be considered for next iterations
    top_n: int
    # Number of rounds of mutation to be performed when generating different styles
    mutation_rounds: int
    # Refine instruction post mutation
    refine_instruction: bool
    # Number of iterations for conducting <mutation_rounds> rounds of mutation of task description
    # followed by refinement of instructions
    mutate_refine_iterations: int
    # Number of iterations for refining task description and in context examples for few-shot
    refine_task_eg_iterations: int
    # Description of task. This will be fed to prompt
    task_description: str
    # Base instruction, in line with your dataset. This will be fed to prompt
    base_instruction: str
    # Instruction for specifying answer format
    answer_format: str
    # Number of samples from dataset, set aside as training data. In every iteration we would be drawing
    # `questions_batch_size` examples from training data with replacement.
    seen_set_size: int
    # Number of examples to be given for few shots
    few_shot_count: int
    # Generate synthetic reasoning
    generate_reasoning: bool
    # Generate description of an expert which can solve the task at hand
    generate_expert_identity: bool
    # Generate keywords that describe the intent of the task
    generate_intent_keywords: bool
    # number of synthetic training examples to be generated
    num_train_examples: int

@dataclass
class PromptPool:
    """Base class for prompt templates."""
    final_prompt: str
    eval_prompt: str
    system_prompt: str

@dataclass
class CritiqueNRefinePromptPool(PromptPool):
    """Prompt templates for critique and refinement."""
    quest_reason_ans: str
    expert_profile: str
    ans_delimiter_instruction: str
    thinking_styles: List[str]
    meta_critique_template: str
    meta_positive_critique_template: str
    critique_refine_template: str
    solve_template: str
    meta_sample_template: str
    intent_template: str
    expert_template: str
    generate_reason_template: str
    reason_optimization_template: str
    examples_optimization_template: str
    examples_critique_template: str = ""
    examples_critique_template_zero_shot: str = ""

# ===============================================================================
# LLM Manager
# ===============================================================================

class LLMMgr(LLMManager):
    """Manager for LLM API calls."""
    
    @staticmethod
    def chat_completion(messages: List[Dict[str, str]]) -> str:
        """
        Make a chat completion request to the OpenAI API.
        
        :param messages: List of message dictionaries with 'role' and 'content'
        :return: The generated text response
        """
        # Create a temporary instance of LLMManager to use its chat_completion method
        llm_manager = LLMManager()
        return llm_manager.chat_completion(messages)

# ===============================================================================
# Core Critique and Refine Logic
# ===============================================================================

class CritiqueNRefine(BasePromptGenerator):
    """
    Implementation of the Critique and Refine method for prompt optimization.
    This class handles generating critiques, refining prompts based on feedback,
    and working with examples to improve prompt performance.
    """

    class GetPromptScoreIndex:
        """
        Class to hold constants. Output of get_prompt_score() method is a list.
        This class stores mapping between output entity and its index in output of get_prompt_score() method.
        """
        PROMPT_STR = 0
        SCORE = 1
        DATASET = 2

    def __init__(self, dataset: List, prompt_pool: CritiqueNRefinePromptPool, data_processor: DatasetSpecificProcessing):
        """
        Initialize the CritiqueNRefine class.
        
        :param dataset: List of examples to use for evaluation
        :param prompt_pool: Object containing prompt templates
        :param data_processor: Object for dataset-specific processing
        """
        # Initialize the BasePromptGenerator with the LLM manager and data processor
        super().__init__(LLMMgr(), data_processor)
        self.dataset = dataset
        self.data_processor = data_processor
        self.prompt_pool = prompt_pool

    def chat_completion(self, user_prompt: str, system_prompt: str = None) -> str:
        """
        Make a chat completion request to the OpenAI API.

        :param user_prompt: Text spoken by user in a conversation.
        :param system_prompt: Text spoken by system in a conversation.
        :return: Output of LLM
        """
        if not system_prompt:
            system_prompt = self.prompt_pool.system_prompt
            
        return super().chat_completion(user_prompt, system_prompt)

    def gen_different_styles(self, base_instruction: str, task_description: str,
                           mutation_rounds: int = 2, thinking_styles_count: int = 10) -> List[str]:
        """
        Generate different variations of base_instruction by mixing thinking styles.

        :param base_instruction: Instruction given to LLM to solve the task defined in task_description.
        :param task_description: Description of the task to be solved.
        :param mutation_rounds: Number of rounds of mutation to be performed when generating different styles.
        :param thinking_styles_count: Number of different thinking styles descriptions to be taken from the pool of
                                    thinking styles and given to LLM as reference (in context).

        :return: List of prompts generated in `mutation_rounds` rounds of mutation.
        """
        candidate_prompts = [task_description + "\n" + base_instruction]

        for mutation_round in range(mutation_rounds):
            mutated_sample_prompt = self.prompt_pool.meta_sample_template.format(
                task_description=task_description,
                meta_prompts="\n".join(self.prompt_pool.thinking_styles[:thinking_styles_count]),
                num_variations=thinking_styles_count,
                prompt_instruction=base_instruction)
            generated_mutated_prompt = self.chat_completion(mutated_sample_prompt)
            # Find all matches of the pattern in the text
            matches = re.findall(self.data_processor.TEXT_DELIMITER_PATTERN_MUTATION, generated_mutated_prompt)
            candidate_prompts.extend(matches)

            print(f"Mutation round {mutation_round} completed")

        return candidate_prompts

    def critique_and_refine(self, prompt: str, critique_example_set: List,
                          further_enhance: bool = False) -> str:
        """
        For the given prompt and examples, generate critique using LLM. 
        Then using the generated critique, refine the prompt using LLM.

        :param prompt: Initial prompt
        :param critique_example_set: Set of examples to be given in context (as few shots)
        :param further_enhance: True if the initial prompt gave number of correct answers more than expected threshold.
                              i.e. we try to further optimize already good prompt.
                              False if the initial prompt gave number of correct answers less than expected
                              threshold. i.e. we try to improve poorly performing prompt.
        :return: refined prompt
        """
        example_string = self.data_processor.collate_to_str(critique_example_set,
                                                          self.prompt_pool.quest_reason_ans)

        if further_enhance:
            # Prompt to get critique on the prompt for which we got the examples right
            meta_critique_prompt = self.prompt_pool.meta_positive_critique_template
        else:
            # Prompt to get critique on the prompt for which we got the examples wrong
            meta_critique_prompt = self.prompt_pool.meta_critique_template

        meta_critique_prompt = meta_critique_prompt.format(instruction=prompt, examples=example_string)

        critique_text = self.chat_completion(meta_critique_prompt, self.prompt_pool.expert_profile)
        critique_refine_prompt = self.prompt_pool.critique_refine_template.format(instruction=prompt,
                                                                                examples=example_string,
                                                                                critique=critique_text,
                                                                                steps_per_sample=1)

        refined_prompts = self.chat_completion(critique_refine_prompt, self.prompt_pool.expert_profile)
        
        refined_prompts = re.findall(self.data_processor.TEXT_DELIMITER_PATTERN, refined_prompts)
        
        if refined_prompts:
            final_refined_prompts = refined_prompts[0]
        else:
            raise ValueError("The LLM output is not in the expected format. Please rerun the code...")

        print(f"Critique received from LLM: {critique_text[:200]}...")
        print(f"Refined prompt: {final_refined_prompts[:200]}...")

        return final_refined_prompts

    def get_prompt_score(self, instructions: List[str], params: PromptOptimizationParams) -> List:
        """
        For each of the prompts in input, make LLM answer a set questions from dataset.
        Check if the answers are correct. Assign score to each prompt based on the number of batches of questions
        answered correctly. Once you get a prompt that gets all the questions right, you can stop the process.

        :params instructions: Prompts using which we'll try to solve the task
        :params params: Object of PromptOptimizationParams class, that has hyperparameters related to prompt
        optimization technique in context.
        :return: A tuple with (Prompt string,
                             score corresponding to that prompt,
                             set of examples over which we evaluated)
        """
        prompt_score_list = []

        for instruction in instructions:
            correct_count, count = 0, 0
            critique_example_set = []
            dataset_subset = random.sample(self.dataset, params.questions_batch_size)
            questions_pool = [example[self.data_processor.QUESTION_LITERAL] for example in dataset_subset]
            while not critique_example_set and \
                    correct_count < params.min_correct_count and \
                    count < params.max_eval_batches:
                count += 1
                solve_prompt = self.prompt_pool.solve_template.format(
                    questions_batch_size=params.questions_batch_size,
                    answer_format=params.answer_format,
                    instruction=instruction,
                    questions='\n'.join(questions_pool))
                
                generated_text = self.chat_completion(solve_prompt)
                critique_example_set = self.evaluate(generated_text, dataset_subset)
                if not critique_example_set:
                    # If all the questions were answered correctly, then we need to get a new set of questions to answer
                    dataset_subset = random.sample(self.dataset, params.questions_batch_size)
                    questions_pool = [example[self.data_processor.QUESTION_LITERAL] for example in dataset_subset]
                    correct_count += 1
                
                print(f"Evaluation batch {count}: {'All correct' if not critique_example_set else f'{len(critique_example_set)} incorrect'}")
            
            print(f"Evaluation completed for instruction. Score: {correct_count}/{count}")
            prompt_score_list.append([instruction, correct_count/count, dataset_subset])

        return prompt_score_list

    def refine_prompts(self, prompt_score_list: List, params: PromptOptimizationParams) -> List[str]:
        """
        Further refine the prompts differently based on whether they got the subset of questions right or wrong.

        :param prompt_score_list: List of (prompt string, score for that prompt string,
        set of examples given in context)
        :param params: Object of class having hyperparameters for Prompt Optimization.
        :return: List of prompts, which were refined over input prompts.
        """
        refined_prompts = []
        for prompt, score, critique_example_set in prompt_score_list:
            if score >= params.min_correct_count/params.max_eval_batches:
                # if it's good enough prompt, how to mutate on that
                refined_prompts.append(self.critique_and_refine(prompt, critique_example_set, True))
            else:
                # if it's not good enough prompt, how to mutate on that
                refined_prompts.append(self.critique_and_refine(prompt, critique_example_set))

        return refined_prompts

    def evaluate(self, generated_text: str, dataset_subset: List) -> List:
        """
        Compare predicted answers with actual answers from the dataset.
        Return the list of questions for which the predicted answer was wrong.

        :param generated_text: Output of LLM, that has answers for a mini-batch of questions
                             (which were send in single go)
        :param dataset_subset: List of examples with question and ground truth.
        :return: List of examples that were wrongly classified.
        """
        # Find all matches of the pattern in the text
        answer_matches = re.findall(self.data_processor.ANSWER_DELIMITER_PATTERN, generated_text)
 
        # If no specific answer pattern found, use the whole text
        if not answer_matches:
            answer_matches = [generated_text]
        
        answers_len, dataset_len = len(answer_matches), len(dataset_subset)
        if answers_len != dataset_len:
            print(f"Warning: Answers extracted ({answers_len}) doesn't match questions asked ({dataset_len})")
            if answers_len > dataset_len:
                # Select last `dataset_len` number of extractions as final.
                answer_matches = answer_matches[-dataset_len:]

        wrong_examples = []
        for i in range(min(answers_len, dataset_len)):
            actual_answer = dataset_subset[i][self.data_processor.FINAL_ANSWER_LITERAL]
            question = dataset_subset[i][self.data_processor.QUESTION_LITERAL]
            is_correct, _ = self.data_processor.access_answer(answer_matches[i], actual_answer)
            if not is_correct:
                wrong_examples.append(dataset_subset[i])
        
        return wrong_examples

    def select_top_prompts(self, prompt_score_list: List, top_n: int) -> List:
        """
        Sort prompts in prompt_score_list, based on its performance. And return max, top `top_n` prompts.

        :param prompt_score_list: List of (prompt string, score for that prompt string,
        set of examples given in context)
        :param top_n: Max number of prompts from the top of the list, that we need to return
        :return: List of top `top_n` prompts.
        """
        sorted_prompts = sorted(prompt_score_list, key=lambda x: [x[self.GetPromptScoreIndex.SCORE],
                                                                len(x[self.GetPromptScoreIndex.PROMPT_STR])],
                              reverse=True)
        sorted_top_n_prompts = sorted_prompts[:top_n]
        return sorted_top_n_prompts

    def extract_examples_frm_response(self, response_with_examples: str) -> List:
        """
        Extract the elements that constitute an example in dataset viz question, reasoning for answer and the answer.
        Put these elements to list and return.

        :param response_with_examples: Response of LLM which has synthetic examples.
        :return: A list of synthetic examples
        """
        synthetic_examples = []
        parsed_data = re.findall(self.data_processor.TEXT_DELIMITER_PATTERN, response_with_examples, re.DOTALL)
        parsed_data = [s.strip() for s in parsed_data]

        for text in parsed_data:
            # Splitting text into question, reason, and answer
            if self.data_processor.QUESTION_KEY_IN_PROMPT in text and \
               self.data_processor.ANSWER_KEY_IN_PROMPT in text:
                question = text[text.find(self.data_processor.QUESTION_KEY_IN_PROMPT) +
                              len(self.data_processor.QUESTION_KEY_IN_PROMPT):
                              text.find(self.data_processor.ANSWER_KEY_IN_PROMPT)].strip()
                answer_with_reason = text[text.find(self.data_processor.ANSWER_KEY_IN_PROMPT) +
                                        len(self.data_processor.ANSWER_KEY_IN_PROMPT):].strip()

                if self.data_processor != None:
                    final_answer = self.data_processor.extract_final_answer(answer_with_reason)
                else:
                    final_answer = extract_between(text=answer_with_reason, start="<ANS_START>", end="<ANS_END>")

                formatted_data = {
                    self.data_processor.QUESTION_LITERAL: question,
                    self.data_processor.ANSWER_WITH_REASON_LITERAL: answer_with_reason,
                    self.data_processor.FINAL_ANSWER_LITERAL: final_answer
                }

                synthetic_examples.append(formatted_data)

        return synthetic_examples

    def generate_reasoning(self, task_description: str, instruction: str, question: str, answer: str) -> str:
        """
        For the given question return the reasoning that's needed to arrive at the provided answer

        :param task_description: Task description of the given task
        :param instruction: Instruction given to LLM for solving the given task
        :param question: Question from the task to be solved
        :param answer: Answer to the question
        :return: Reasoning that went through for getting answer `answer` for question `question`
        """
        prompt_template = self.prompt_pool.generate_reason_template.format(task_description=task_description,
                                                                         instruction=instruction,
                                                                         question=question,
                                                                         answer=answer)
        return self.chat_completion(user_prompt=prompt_template)

    def generate_expert_identity(self, task_description: str) -> str:
        """
        Generate sentence using LLM, describing the identity of an expert, who is apt to solve the task defined
        in task_description
        :param task_description: Task description of the given task
        :return: An expert profile, that can go in as system prompt and LLM would be asked to act as per this
        expert profile.
        """
        return super().generate_expert_identity(task_description, self.prompt_pool.expert_template)

    def generate_intent_keywords(self, task_description: str, instruction: str) -> str:
        """
        For a given task description and instruction, generate keywords that describe the intent.

        :param task_description: Description of the task that has to be solved by LLM
        :param instruction: Instruction given to LLM for solving the given task
        """
        return super().generate_intent_keywords(task_description, instruction, self.prompt_pool.intent_template)

    def generate_best_examples(self, examples: List, params: PromptOptimizationParams) -> List:
        """
        Generate best example to be give as few-shots for the given task.

        :param examples: List of examples. Each example is a dictionary with keys as question/reason/answer
        :param params: Object having hyperparameters for this prompt optimization technique.
        :return: List of synthetic examples
        """
        example_string = self.data_processor.collate_to_str(examples, self.prompt_pool.quest_reason_ans)
        
        # If examples_critique_template is not provided, use a default approach
        if not hasattr(self.prompt_pool, 'examples_critique_template') or not self.prompt_pool.examples_critique_template:
            # Generate critique directly using meta_critique_template
            critique = self.chat_completion(
                self.prompt_pool.meta_critique_template.format(
                    instruction=params.base_instruction,
                    examples=example_string
                ),
                self.prompt_pool.expert_profile
            )
        else:
            # Use the provided examples_critique_template
            few_shot_critique_prompt = self.prompt_pool.examples_critique_template.format(
                prompt=params.base_instruction,
                examples=example_string,
                task_description=params.task_description
            )
            critique = self.chat_completion(few_shot_critique_prompt, self.prompt_pool.expert_profile)

        # Get a sample example to use as ground truth example
        gt_example = ""
        if examples:
            sample_example = examples[0]
            gt_example = f"{self.data_processor.QUESTION_KEY_IN_PROMPT} {sample_example[self.data_processor.QUESTION_LITERAL]}\n"
            gt_example += f"{self.data_processor.ANSWER_KEY_IN_PROMPT} {sample_example[self.data_processor.ANSWER_WITH_REASON_LITERAL]}"

        few_shot_opt_prompt = self.prompt_pool.examples_optimization_template.format(
            prompt=params.base_instruction,
            examples=example_string,
            gt_example=gt_example,
            critique=critique,
            task_description=params.task_description,
            num_examples=params.few_shot_count
        )
        
        synthetic_examples = self.chat_completion(few_shot_opt_prompt, self.prompt_pool.expert_profile)
        return self.extract_examples_frm_response(synthetic_examples)

    def get_best_prompt(self, params: PromptOptimizationParams, use_examples=False) -> Tuple[str, Any]:
        """
        Perform `params.max_iterations` iterations for optimizing your prompt. And return the best prompt found so far.

        :params: Object of class PromptOptimizationParams, that has all hyper-parameters needed for prompt optimization.
        :param use_examples: Whether to use examples in the optimization process
        :return: Best prompt for the given task and dataset.
        """
        current_base_instruction = params.base_instruction
        best_prompt = current_base_instruction
        best_score = 0
        best_examples = []

        # Generate expert identity if requested
        expert_identity = None
        if params.generate_expert_identity:
            expert_identity = self.generate_expert_identity(params.task_description)
            self.prompt_pool.expert_profile = expert_identity
            print(f"Generated expert identity: {expert_identity[:100]}...")

        # Generate intent keywords if requested
        intent_keywords = None
        if params.generate_intent_keywords:
            intent_keywords = self.generate_intent_keywords(params.task_description, current_base_instruction)
            print(f"Generated intent keywords: {intent_keywords[:100]}...")

        # Mutate and refine iterations
        for iteration in range(params.mutate_refine_iterations):
            print(f"\nStarting mutation-refinement iteration {iteration+1}/{params.mutate_refine_iterations}")
            
            # Generate different styles of prompts
            candidate_prompts = self.gen_different_styles(
                base_instruction=current_base_instruction,
                task_description=params.task_description,
                mutation_rounds=params.mutation_rounds,
                thinking_styles_count=params.style_variation
            )
            
            # Evaluate prompts and get scores
            prompt_score_list = self.get_prompt_score(candidate_prompts, params)
            
            # Select top performing prompts
            top_prompts = self.select_top_prompts(prompt_score_list, params.top_n)
            
            # Update best prompt if we found a better one
            if top_prompts and top_prompts[0][self.GetPromptScoreIndex.SCORE] > best_score:
                best_prompt = top_prompts[0][self.GetPromptScoreIndex.PROMPT_STR]
                best_score = top_prompts[0][self.GetPromptScoreIndex.SCORE]
                best_examples = top_prompts[0][self.GetPromptScoreIndex.DATASET]
            
            # Refine the prompts if requested
            if params.refine_instruction:
                refined_prompts = self.refine_prompts(top_prompts, params)
                current_base_instruction = refined_prompts[0] if refined_prompts else current_base_instruction
            else:
                current_base_instruction = top_prompts[0][self.GetPromptScoreIndex.PROMPT_STR] if top_prompts else current_base_instruction
            
            print(f"Iteration {iteration+1} completed. Best score so far: {best_score}")

        # Refine with examples if requested
        if use_examples and params.few_shot_count > 0:
            print("\nStarting example-based refinement")
            
            for iteration in range(params.refine_task_eg_iterations):
                print(f"Example refinement iteration {iteration+1}/{params.refine_task_eg_iterations}")
                
                # Generate synthetic examples if needed
                if not best_examples or len(best_examples) < params.few_shot_count:
                    # If we don't have enough examples, generate more
                    synthetic_examples = self.generate_best_examples(
                        examples=best_examples if best_examples else random.sample(self.dataset, min(params.few_shot_count, len(self.dataset))),
                        params=params
                    )
                    
                    # Add reasoning to examples if requested
                    if params.generate_reasoning:
                        for example in synthetic_examples:
                            if self.data_processor.ANSWER_WITH_REASON_LITERAL not in example:
                                reasoning = self.generate_reasoning(
                                    task_description=params.task_description,
                                    instruction=best_prompt,
                                    question=example[self.data_processor.QUESTION_LITERAL],
                                    answer=example[self.data_processor.FINAL_ANSWER_LITERAL]
                                )
                                example[self.data_processor.ANSWER_WITH_REASON_LITERAL] = reasoning
                    
                    best_examples = synthetic_examples[:params.few_shot_count]
                
                # Create prompt with examples
                example_string = self.data_processor.collate_to_str(best_examples, self.prompt_pool.quest_reason_ans)
                prompt_with_examples = f"{best_prompt}\n\nHere are some examples:\n{example_string}"
                
                # Evaluate the prompt with examples
                prompt_score = self.get_prompt_score([prompt_with_examples], params)
                
                if prompt_score and prompt_score[0][self.GetPromptScoreIndex.SCORE] > best_score:
                    best_prompt = prompt_with_examples
                    best_score = prompt_score[0][self.GetPromptScoreIndex.SCORE]
                    print(f"Found better prompt with examples. New score: {best_score}")
                
                # Refine examples for next iteration
                best_examples = self.generate_best_examples(best_examples, params)
        
        # Final evaluation
        final_prompt = best_prompt
        print(f"\nOptimization completed. Final best score: {best_score}")
        print(f"Final prompt: {final_prompt[:200]}...")
        
        return final_prompt, best_score
